{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJjJFDAuJTbNst1OVX8XXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirtualGoat/Enhanced-CCTV-security-using-DL/blob/master/Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IYk_I5dc-ut",
        "colab_type": "code",
        "outputId": "c2ea1b83-8490-4339-e021-99a182947952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!pip install imageAI"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting imageAI\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/99/4023e191a343fb23f01ae02ac57a5ca58037c310e8d8c62f87638a3bafc7/imageai-2.1.5-py3-none-any.whl (180kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 24.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 92kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageAI) (6.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imageAI) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imageAI) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageAI) (1.17.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from imageAI) (2.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->imageAI) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imageAI) (42.0.2)\n",
            "Installing collected packages: imageAI\n",
            "Successfully installed imageAI-2.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l3cy2s9dDCd",
        "colab_type": "code",
        "outputId": "fc9681af-d3b0-4a6e-ecf6-48a0d6db88ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVVcitDIfn_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forFrame(frame_number, output_array, output_count, detected_copy):\n",
        "    print(\"FOR FRAME \" , frame_number)\n",
        "#    print(\"Output for each object : \", output_array)\n",
        "    print(\"Output count for unique objects : \", output_count)\n",
        "    print(\"------------END OF A FRAME --------------\")\n",
        "\n",
        "def forSeconds(second_number, output_arrays, count_arrays, average_output_count, detected_copy):\n",
        "    print(\"SECOND : \", second_number)\n",
        "#    print(\"Array for the outputs of each frame \", output_arrays)\n",
        "    print(\"Array for output count for unique objects in each frame : \", count_arrays)\n",
        "    print(\"Output average count for unique objects in the last second: \", average_output_count)\n",
        "    print(\"------------END OF A SECOND --------------\")\n",
        "\n",
        "def forMinute(minute_number, output_arrays, count_arrays, average_output_count, detected_copy):\n",
        "    print(\"MINUTE : \", minute_number)\n",
        "#    print(\"Array for the outputs of each frame \", output_arrays)\n",
        "    print(\"Array for output count for unique objects in each frame : \", count_arrays)\n",
        "    print(\"Output average count for unique objects in the last minute: \", average_output_count)\n",
        "    print(\"------------END OF A MINUTE --------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dQkm4HldEju",
        "colab_type": "code",
        "outputId": "58a343b6-9d3e-43b5-ba57-eb98e8f6c92b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yolo_path = '/content/drive/My Drive/Colab Notebooks/ImageAI/'\n",
        "video_path = '/content/drive/My Drive/Colab Notebooks/ImageAI/'\n",
        "execution_path = os.getcwd()\n",
        "print(execution_path)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9OTeecRWRlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "eb087647-a15e-4d9d-9b99-cf5026a25c1a"
      },
      "source": [
        "pip install utils"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/66/cc/276bcc98fb2d1e609c6c2230cc9ad76a3a29839f79c91e608cfb347d6ad7/utils-1.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5N8UUz1Vwrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "84eb5aff-4efb-4f13-ab5e-9c7f4a4a1688"
      },
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2 \n",
        "from utils import *\n",
        "from DNModel import net as Darknet\n",
        "from img_process import inp_to_image, custom_resize\n",
        "import pandas as pd\n",
        "import random \n",
        "import pickle as pkl\n",
        "import argparse\n",
        "\n",
        "\n",
        "\n",
        "def prepare_input(img, inp_dim):\n",
        "    \"\"\"\n",
        "    Prepare image for inputting to the neural network. \n",
        "    Perform tranpose and return Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    orig_im = img\n",
        "    dim = orig_im.shape[1], orig_im.shape[0]\n",
        "    img = (custom_resize(orig_im, (inp_dim, inp_dim)))\n",
        "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
        "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
        "    return img_, orig_im, dim\n",
        "\n",
        "def write(x, img):\n",
        "    c1 = tuple(x[1:3].int())\n",
        "    c2 = tuple(x[3:5].int())\n",
        "    cls = int(x[-1])\n",
        "    label = \"{0}\".format(classes[cls])\n",
        "    color = random.choice(colors)\n",
        "    cv2.rectangle(img, c1, c2,color, 1)\n",
        "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
        "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
        "    cv2.rectangle(img, c1, c2,color, -1)\n",
        "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
        "    return img\n",
        "\n",
        "def arg_parse():\n",
        "    \"\"\"\n",
        "    Parse arguements to the detect module\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
        "   \n",
        "    parser.add_argument(\"--video\", dest = 'video', help = \n",
        "                        \"Video to run detection upon\",\n",
        "                        default = \"video.avi\", type = str)\n",
        "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
        "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
        "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
        "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n",
        "                        \"Config file\",\n",
        "                        default = \"cfg/yolov3.cfg\", type = str)\n",
        "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
        "                        \"weightsfile\",\n",
        "                        default = \"yolov3.weights\", type = str)\n",
        "    parser.add_argument(\"--reso\", dest = 'reso', help = \n",
        "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
        "                        default = \"128\", type = str)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = arg_parse()\n",
        "    confidence = float(args.confidence)\n",
        "    nms_thesh = float(args.nms_thresh)\n",
        "    start = 0\n",
        "\n",
        "    CUDA = torch.cuda.is_available()\n",
        "\n",
        "    num_classes = 80\n",
        "    \n",
        "    bbox_attrs = 5 + num_classes\n",
        "    \n",
        "    print(\"Loading network\")\n",
        "    model = Darknet(args.cfgfile)\n",
        "    model.load_weights(args.weightsfile)\n",
        "    print(\"Network loaded\")\n",
        "    classes = load_classes('data/coco.names')\n",
        "    colors = pkl.load(open(\"pallete\", \"rb\"))\n",
        "    model.DNInfo[\"height\"] = args.reso\n",
        "    inp_dim = int(model.DNInfo[\"height\"])\n",
        "\n",
        "\n",
        "    if CUDA:\n",
        "        model.cuda()\n",
        "        \n",
        "    model.eval()\n",
        "    \n",
        "    videofile = '/content/drive/My Drive/Colab Notebooks/ImageAI/messi.mp4'\n",
        "    \n",
        "    cap = cv2.VideoCapture(videofile)\n",
        "    \n",
        "    assert cap.isOpened(), 'Cannot capture source'\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        \n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            \n",
        "\n",
        "            img, orig_im, dim = prepare_input(frame, inp_dim)\n",
        "            \n",
        "            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
        "            \n",
        "            \n",
        "            if CUDA:\n",
        "                im_dim = im_dim.cuda()\n",
        "                img = img.cuda()\n",
        "            \n",
        "            with torch.no_grad():   \n",
        "                output = model(Variable(img), CUDA)\n",
        "            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
        "\n",
        "            if type(output) == int:\n",
        "                cv2.imshow(\"frame\", orig_im)\n",
        "                key = cv2.waitKey(1)\n",
        "                if key & 0xFF == ord('x'):\n",
        "                    break\n",
        "                continue\n",
        "        \n",
        "            \n",
        "            im_dim = im_dim.repeat(output.size(0), 1)\n",
        "            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
        "            \n",
        "            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
        "            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
        "            \n",
        "            output[:,1:5] /= scaling_factor\n",
        "    \n",
        "            for i in range(output.shape[0]):\n",
        "                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
        "                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
        "            \n",
        "\n",
        "            \n",
        "            list(map(lambda x: write(x, orig_im), output))\n",
        "            \n",
        "            \n",
        "            cv2.imshow(\"frame\", orig_im)\n",
        "            key = cv2.waitKey(1)\n",
        "            if key & 0xFF == ord('x'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-02cc735a3e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDNModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimg_process\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minp_to_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DNModel'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4jGivJ5fsfo",
        "colab_type": "code",
        "outputId": "b569fc02-704a-4065-f419-7daa5ea7a19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "use_model = \"yolo.h5\"\n",
        "use_video=\"cctv.mp4\"\n",
        "from imageai.Detection import VideoObjectDetection\n",
        "import cv2\n",
        "analisys_video = \"detected_\" + use_video\n",
        "\n",
        "detector = VideoObjectDetection()\n",
        "camera = cv2.VideoCapture(\"http://186.195.206.250:8080/view/index.shtml\")\n",
        "# Iniciate model\n",
        "detector.setModelTypeAsYOLOv3()      # To YOLOv3 model\n",
        "#detector.setModelTypeAsRetinaNet()   # To Retinanet model\n",
        "#detector.setModelTypeAsTinyYOLOv3()   # To TinyYOLOv3 model\n",
        "\n",
        "detector.setModelPath( os.path.join(yolo_path , use_model))\n",
        "detector.loadModel()\n",
        "\n",
        "vid_path = detector.detectObjectsFromVideo(camera_input=camera,\n",
        "                                         # input_file_path=os.path.join( video_path, use_video),\n",
        "                                           output_file_path=os.path.join(video_path, analisys_video),\n",
        "                                           frames_per_second=30,\n",
        "                                 #          per_second_function = forSeconds,\n",
        "                                           per_frame_function = forFrame,\n",
        "                                 #          per_minute_function= forMinute,\n",
        "                                           minimum_percentage_probability=30,\n",
        "                                           log_progress=True,\n",
        "                                           return_detected_frame=True)\n",
        "                                           \n",
        "\n",
        "print(vid_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/content/drive/My Drive/Colab Notebooks/ImageAI/detected_cctv.mp4.avi\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}