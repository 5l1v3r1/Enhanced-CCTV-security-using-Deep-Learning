{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOpRE9w+36sIrld14rwWd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirtualGoat/Enhanced-CCTV-security-using-Deep-Learning/blob/master/Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IYk_I5dc-ut",
        "colab_type": "code",
        "outputId": "4f434948-c932-447d-8f67-1b18c3d4f903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "!pip install imageAI"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageAI in /usr/local/lib/python3.6/dist-packages (2.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imageAI) (3.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from imageAI) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageAI) (1.17.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageAI) (6.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imageAI) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageAI) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->imageAI) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imageAI) (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l3cy2s9dDCd",
        "colab_type": "code",
        "outputId": "14938587-ee4f-47dd-bc01-8981ad419cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVVcitDIfn_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forFrame(frame_number, output_array, output_count, detected_copy):\n",
        "    print(\"FOR FRAME \" , frame_number)\n",
        "#    print(\"Output for each object : \", output_array)\n",
        "    print(\"Output count for unique objects : \", output_count)\n",
        "    print(\"------------END OF A FRAME --------------\")\n",
        "\n",
        "def forSeconds(second_number, output_arrays, count_arrays, average_output_count, detected_copy):\n",
        "    print(\"SECOND : \", second_number)\n",
        "#    print(\"Array for the outputs of each frame \", output_arrays)\n",
        "    print(\"Array for output count for unique objects in each frame : \", count_arrays)\n",
        "    print(\"Output average count for unique objects in the last second: \", average_output_count)\n",
        "    print(\"------------END OF A SECOND --------------\")\n",
        "\n",
        "def forMinute(minute_number, output_arrays, count_arrays, average_output_count, detected_copy):\n",
        "    print(\"MINUTE : \", minute_number)\n",
        "#    print(\"Array for the outputs of each frame \", output_arrays)\n",
        "    print(\"Array for output count for unique objects in each frame : \", count_arrays)\n",
        "    print(\"Output average count for unique objects in the last minute: \", average_output_count)\n",
        "    print(\"------------END OF A MINUTE --------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dQkm4HldEju",
        "colab_type": "code",
        "outputId": "1e13cab9-7045-4730-9b40-402633b42dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yolo_path = '/content/drive/My Drive/Colab Notebooks/ImageAI/'\n",
        "video_path = '/content/drive/My Drive/Colab Notebooks/ImageAI/'\n",
        "execution_path = os.getcwd()\n",
        "print(execution_path)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9OTeecRWRlJ",
        "colab_type": "code",
        "outputId": "eb087647-a15e-4d9d-9b99-cf5026a25c1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "pip install utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/66/cc/276bcc98fb2d1e609c6c2230cc9ad76a3a29839f79c91e608cfb347d6ad7/utils-1.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5N8UUz1Vwrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2 \n",
        "from utils import *\n",
        "from DNModel import net as Darknet\n",
        "from img_process import inp_to_image, custom_resize\n",
        "import pandas as pd\n",
        "import random \n",
        "import pickle as pkl\n",
        "import argparse\n",
        "\n",
        "\n",
        "\n",
        "def prepare_input(img, inp_dim):\n",
        "    \"\"\"\n",
        "    Prepare image for inputting to the neural network. \n",
        "    Perform tranpose and return Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    orig_im = img\n",
        "    dim = orig_im.shape[1], orig_im.shape[0]\n",
        "    img = (custom_resize(orig_im, (inp_dim, inp_dim)))\n",
        "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
        "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
        "    return img_, orig_im, dim\n",
        "\n",
        "def write(x, img):\n",
        "    c1 = tuple(x[1:3].int())\n",
        "    c2 = tuple(x[3:5].int())\n",
        "    cls = int(x[-1])\n",
        "    label = \"{0}\".format(classes[cls])\n",
        "    color = random.choice(colors)\n",
        "    cv2.rectangle(img, c1, c2,color, 1)\n",
        "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
        "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
        "    cv2.rectangle(img, c1, c2,color, -1)\n",
        "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
        "    return img\n",
        "\n",
        "def arg_parse():\n",
        "    \"\"\"\n",
        "    Parse arguements to the detect module\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
        "   \n",
        "    parser.add_argument(\"--video\", dest = 'video', help = \n",
        "                        \"Video to run detection upon\",\n",
        "                        default = \"video.avi\", type = str)\n",
        "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
        "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
        "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
        "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n",
        "                        \"Config file\",\n",
        "                        default = \"cfg/yolov3.cfg\", type = str)\n",
        "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
        "                        \"weightsfile\",\n",
        "                        default = \"yolov3.weights\", type = str)\n",
        "    parser.add_argument(\"--reso\", dest = 'reso', help = \n",
        "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
        "                        default = \"128\", type = str)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = arg_parse()\n",
        "    confidence = float(args.confidence)\n",
        "    nms_thesh = float(args.nms_thresh)\n",
        "    start = 0\n",
        "\n",
        "    CUDA = torch.cuda.is_available()\n",
        "\n",
        "    num_classes = 80\n",
        "    \n",
        "    bbox_attrs = 5 + num_classes\n",
        "    \n",
        "    print(\"Loading network\")\n",
        "    model = Darknet(args.cfgfile)\n",
        "    model.load_weights(args.weightsfile)\n",
        "    print(\"Network loaded\")\n",
        "    classes = load_classes('data/coco.names')\n",
        "    colors = pkl.load(open(\"pallete\", \"rb\"))\n",
        "    model.DNInfo[\"height\"] = args.reso\n",
        "    inp_dim = int(model.DNInfo[\"height\"])\n",
        "\n",
        "\n",
        "    if CUDA:\n",
        "        model.cuda()\n",
        "        \n",
        "    model.eval()\n",
        "    \n",
        "    videofile = '/content/drive/My Drive/Colab Notebooks/ImageAI/messi.mp4'\n",
        "    \n",
        "    cap = cv2.VideoCapture(videofile)\n",
        "    \n",
        "    assert cap.isOpened(), 'Cannot capture source'\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        \n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            \n",
        "\n",
        "            img, orig_im, dim = prepare_input(frame, inp_dim)\n",
        "            \n",
        "            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
        "            \n",
        "            \n",
        "            if CUDA:\n",
        "                im_dim = im_dim.cuda()\n",
        "                img = img.cuda()\n",
        "            \n",
        "            with torch.no_grad():   \n",
        "                output = model(Variable(img), CUDA)\n",
        "            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
        "\n",
        "            if type(output) == int:\n",
        "                cv2.imshow(\"frame\", orig_im)\n",
        "                key = cv2.waitKey(1)\n",
        "                if key & 0xFF == ord('x'):\n",
        "                    break\n",
        "                continue\n",
        "        \n",
        "            \n",
        "            im_dim = im_dim.repeat(output.size(0), 1)\n",
        "            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
        "            \n",
        "            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
        "            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
        "            \n",
        "            output[:,1:5] /= scaling_factor\n",
        "    \n",
        "            for i in range(output.shape[0]):\n",
        "                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
        "                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
        "            \n",
        "\n",
        "            \n",
        "            list(map(lambda x: write(x, orig_im), output))\n",
        "            \n",
        "            \n",
        "            cv2.imshow(\"frame\", orig_im)\n",
        "            key = cv2.waitKey(1)\n",
        "            if key & 0xFF == ord('x'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4jGivJ5fsfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_model = \"yolo.h5\"\n",
        "use_video=\"cctv.mp4\"\n",
        "from imageai.Detection import VideoObjectDetection\n",
        "import cv2\n",
        "analisys_video = \"detected_\" + use_video\n",
        "\n",
        "detector = VideoObjectDetection()\n",
        "camera = cv2.VideoCapture(\"http://186.195.206.250:8080/view/index.shtml\")\n",
        "# Iniciate model\n",
        "detector.setModelTypeAsYOLOv3()      # To YOLOv3 model\n",
        "#detector.setModelTypeAsRetinaNet()   # To Retinanet model\n",
        "#detector.setModelTypeAsTinyYOLOv3()   # To TinyYOLOv3 model\n",
        "\n",
        "detector.setModelPath( os.path.join(yolo_path , use_model))\n",
        "detector.loadModel()\n",
        "\n",
        "vid_path = detector.detectObjectsFromVideo(camera_input=camera,\n",
        "                                         # input_file_path=os.path.join( video_path, use_video),\n",
        "                                           output_file_path=os.path.join(video_path, analisys_video),\n",
        "                                           frames_per_second=30,\n",
        "                                 #          per_second_function = forSeconds,\n",
        "                                           per_frame_function = forFrame,\n",
        "                                 #          per_minute_function= forMinute,\n",
        "                                           minimum_percentage_probability=30,\n",
        "                                           log_progress=True,\n",
        "                                           return_detected_frame=True)\n",
        "                                           \n",
        "\n",
        "print(vid_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jhVo36PHOEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHRPzB8tFVsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2 \n",
        "\n",
        "vidObj = cv2.VideoCapture('/content/drive/My Drive/Colab Notebooks/ImageAI/messi1/messi.mp4') \n",
        "\n",
        "count = 0\n",
        "\n",
        "success = 1\n",
        "\n",
        "while success: \n",
        "\n",
        "  success, image = vidObj.read() \n",
        "  if count%20==0:\n",
        "    cv2.imwrite(\"/content/drive/My Drive/Colab Notebooks/ImageAI/messi1/frame%d.jpg\" % count, image) \n",
        "\n",
        "  count += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0uZO7PdXaG2",
        "colab_type": "code",
        "outputId": "ccc5135d-93a5-40c7-8681-340f6c94a63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# from imageai.Detection import ObjectDetection\n",
        "import os\n",
        "use_model = \"yolo.h5\"\n",
        "use_video=\"messi.mp4\"\n",
        "from imageai.Detection import VideoObjectDetection\n",
        "import cv2\n",
        "analisys_video = \"detected_\" + use_video\n",
        "\n",
        "execution_path = '/content/drive/My Drive/Colab Notebooks/ImageAI/'\n",
        "\n",
        "detector = VideoObjectDetection()\n",
        "detector.setModelTypeAsYOLOv3()\n",
        "detector.setModelPath( os.path.join(execution_path , \"yolo.h5\"))\n",
        "detector.loadModel()\n",
        "\n",
        "\n",
        "video_path = '/content/drive/My Drive/Colab Notebooks/ImageAI/'\n",
        "\n",
        "\n",
        "detections, objects_path = detector.detectObjectsFromVideo(\n",
        "                                           input_file_path=os.path.join( video_path, use_video),\n",
        "                                           output_file_path=os.path.join(video_path, analisys_video),\n",
        "                                           frames_per_second=30,\n",
        "                                 #          per_second_function = forSeconds,\n",
        "                                           per_frame_function = forFrame,\n",
        "                                 #          per_minute_function= forMinute,\n",
        "                                           minimum_percentage_probability=30,\n",
        "                                           log_progress=True,\n",
        "                                           return_detected_frame=True,\n",
        "                                           extract_detected_objects=True)\n",
        "\n",
        "for eachObject, eachObjectPath in zip(detections, objects_path):\n",
        "    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
        "    print(\"Object's image saved in \" + eachObjectPath)\n",
        "    print(\"--------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7483864499cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                            \u001b[0mlog_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                            \u001b[0mreturn_detected_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                            extract_detected_objects=True)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meachObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meachObjectPath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjects_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: detectObjectsFromVideo() got an unexpected keyword argument 'extract_detected_objects'"
          ]
        }
      ]
    }
  ]
}